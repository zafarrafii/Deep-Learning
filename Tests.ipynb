{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd998c20",
   "metadata": {},
   "source": [
    "# Deep Learning Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28588085",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "https://d2l.ai/chapter_linear-networks/linear-regression-scratch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eab6bb",
   "metadata": {},
   "source": [
    "### 1.1. Linear regression from scratch in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a84b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: 2.877929794325924\n",
      "2/3: 0.00457104134012375\n",
      "3/3: 5.3823265050773114e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = np.array([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = np.random.default_rng().normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = np.matmul(X, w_true)+b_true+np.random.default_rng().normal(0, 0.01, number_examples)\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "\n",
    "# Initialize the weights and bias to recover\n",
    "w = np.random.default_rng().normal(0, 1, number_features)\n",
    "b = 0\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = np.zeros(number_epochs)\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Generate the indices for all the examples and shuffle them\n",
    "    example_indices =  np.arange(number_examples)\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minbatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for j in np.arange(0, number_examples, batch_size):\n",
    "        \n",
    "        # Get the indices of the (randomized) examples for one minibatch\n",
    "        batch_indices = example_indices[j:min(j+batch_size, number_examples)]\n",
    "        \n",
    "        # Get the inputs and outputs for the current minibatch\n",
    "        X_batch = X[batch_indices, :]\n",
    "        y_batch = y[batch_indices]\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = np.matmul(X_batch, w) + b\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = np.mean(0.5*np.power(y_hat-y_batch, 2))\n",
    "        \n",
    "        # Save the mean loss for the current minibatch\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        # Update the weights and bias using stochastic gradient descent\n",
    "        w = w - lr*np.mean(X_batch*(y_hat-y_batch)[:, np.newaxis], axis=0)\n",
    "        b = b - lr*np.mean(y_hat-y_batch, axis=0)\n",
    "        \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = np.mean(batch_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb9d9b",
   "metadata": {},
   "source": [
    "### 1.2. Linear regression from scratch in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01b5b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: 4.321141719818115\n",
      "2/3: 0.014720503240823746\n",
      "3/3: 0.00010053945879917592\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = torch.tensor([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = torch.normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = torch.matmul(X, w_true)+b_true+torch.normal(0, 0.01, [number_examples]) # [number_examples]?\n",
    "\n",
    "# Define a function to read the dataset in random batches\n",
    "def batch(X, y, batch_size):\n",
    "    \n",
    "    # Generate the indices for all the examples and shuffle them\n",
    "    number_examples = X.shape[0]\n",
    "    example_indices = list(range(number_examples))\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for i in range(0, number_examples, batch_size):\n",
    "        \n",
    "        # Get the indices of the (randomized) examples for one minibatch\n",
    "        batch_indices = example_indices[i:min(i+batch_size, number_examples)]\n",
    "        \n",
    "        # Return the input and output minibatch and continue the iteration in the function\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "\n",
    "# Initialize the weights and bias to recover, requiring the gradients to be computed\n",
    "w = torch.normal(0, 1, [number_features], requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = torch.zeros(number_epochs)\n",
    "        \n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minibatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for X_batch, y_batch in batch(X, y, batch_size):\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = torch.matmul(X_batch, w) + b\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = 0.5*(y_hat-y_batch)**2\n",
    "        \n",
    "        # Compute the gradient on l with respect to w and b\n",
    "        # (sum and not mean as the gradients will be divided by the batch size during the SGD)\n",
    "        l.sum().backward()\n",
    "        \n",
    "        # Temporarily sets all of the requires_grad flags to false\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Save the mean loss for the current minibatch\n",
    "            batch_loss.append(l.mean())\n",
    "            \n",
    "            # Update the weights and bias using stochastic gradient descent\n",
    "            # (use augmented assignments to avoid modifying existing variables)\n",
    "            w -= lr*w.grad/len(l)\n",
    "            b -= lr*b.grad/len(l)\n",
    "            \n",
    "            # Set the gradients to zeros to avoid accumulating gradients\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = sum(batch_loss)/len(batch_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27c36f",
   "metadata": {},
   "source": [
    "### 1.3. Linear regression using APIs in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "219ae16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: 2.891758441925049\n",
      "2/3: 0.00010611279139993712\n",
      "3/3: 9.781956759979948e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = torch.tensor([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = torch.normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = torch.matmul(X, w_true)+b_true+torch.normal(0, 0.01, [number_examples]) # [number_examples]?\n",
    "\n",
    "# Define a function to read the dataset in random batches\n",
    "def batch(X, y, batch_size):\n",
    "    \n",
    "    # Construct a PyTorch data iterator (?)\n",
    "    data_set = data.TensorDataset(*(X, y))\n",
    "    return data.DataLoader(data_set, batch_size, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "# - Sequential class: defines container to chain several layers together\n",
    "# - Linear class: defines fully-connected layer (with input and output feature dimensions)\n",
    "model = nn.Sequential(nn.Linear(number_features, 1))\n",
    "\n",
    "# Initialize the parameters\n",
    "# - model[0]: accessing first layer in the model\n",
    "# - weight.data and bias.data to access the parameters\n",
    "model[0].weight.data.normal_(0, 0.01)\n",
    "model[0].bias.data.fill_(0)\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "\n",
    "# Define the loss function (mean squared error, without the 0.5 factor)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Define the optimization algorithm (stochastic gradient descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = torch.zeros(number_epochs)\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minibatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for X_batch, y_batch in batch(X, y, batch_size):\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = model(X_batch)\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = loss(y_hat, y_batch[:, None])\n",
    "        \n",
    "        # Save the loss for the current minibatch (no with torch.no_grad())\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        # Set the gradients to zero (.zero_grad()?)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Computes the gradient (no .sum?)\n",
    "        l.backward()\n",
    "        \n",
    "        # Performs a single parameter update\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = sum(batch_loss)/len(batch_loss)\n",
    "        \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d77f99",
   "metadata": {},
   "source": [
    "### 1.4. Linear regression using higher-level APIs in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b017dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE: Check Keras and print predicted weights and bias everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef4ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 0s 392us/step - loss: 2.8657\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 362us/step - loss: 1.1299e-04\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 362us/step - loss: 1.0238e-04\n",
      "\n",
      "Predicted weights: [ 1.9997882 -3.3989787]\n",
      "Predicted bias: 4.199860572814941\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = np.array([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = np.random.default_rng().normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = np.matmul(X, w_true)+b_true+np.random.default_rng().normal(0, 0.01, number_examples)\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "\n",
    "# Define the model (as a feedforward NN)\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an input with the number of features\n",
    "model.add(tf.keras.Input(shape=number_features))\n",
    "\n",
    "# Add a densely-connected NN layer without activation and with initialized weights and bias\n",
    "model.add(tf.keras.layers.Dense(1, activation=None, \\\n",
    "                                kernel_initializer=tf.initializers.RandomNormal(mean=0, stddev=0.01), \\\n",
    "                                bias_initializer='zeros'))\n",
    "\n",
    "# Configure the model for training with stochastic gradient descent optimizer and mean squared error loss\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "# Train the model give the batch size and number of epochs\n",
    "model.fit(x=X, y=y, batch_size=batch_size, epochs=number_epochs, verbose=1)\n",
    "\n",
    "# Print the predicted weights and bias\n",
    "print(\"\")\n",
    "print(f\"Predicted weights: {model.get_weights()[0][:, 0]}\")\n",
    "print(f\"Predicted bias: {model.get_weights()[1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a81c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
