{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd998c20",
   "metadata": {},
   "source": [
    "# Deep Learning Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28588085",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "https://d2l.ai/chapter_linear-networks/linear-regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eab6bb",
   "metadata": {},
   "source": [
    "### 1.1. Linear regression from scratch in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a84b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: 3.9596817087120337\n",
      "2/3: 0.008516841412285217\n",
      "3/3: 7.255705744279113e-05\n",
      "\n",
      "w = [ 1.99930911 -3.39979572]\n",
      "b = 4.199669463443418\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = np.array([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = np.random.default_rng().normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = np.matmul(X, w_true)+b_true+np.random.default_rng().normal(0, 0.01, number_examples)\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "\n",
    "# Initialize the weights and bias to recover\n",
    "w = np.random.default_rng().normal(0, 1, number_features)\n",
    "b = 0\n",
    "\n",
    "# Initialize a list for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = np.zeros(number_epochs)\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Generate the indices for all the examples and shuffle them\n",
    "    example_indices = np.arange(number_examples)\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minbatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for j in np.arange(0, number_examples, batch_size):\n",
    "        \n",
    "        # Get the indices of the (randomized) examples for one minibatch\n",
    "        batch_indices = example_indices[j:min(j+batch_size, number_examples)]\n",
    "        \n",
    "        # Get the inputs and outputs for the current minibatch\n",
    "        X_batch = X[batch_indices, :]\n",
    "        y_batch = y[batch_indices]\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = np.matmul(X_batch, w) + b\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = np.mean(0.5*np.power(y_hat-y_batch, 2))\n",
    "        \n",
    "        # Save the loss for the current minibatch\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        # Update the weights and bias using stochastic gradient descent\n",
    "        w = w - lr*np.mean(X_batch*(y_hat-y_batch)[:, np.newaxis], axis=0)\n",
    "        b = b - lr*np.mean(y_hat-y_batch, axis=0)\n",
    "        \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = np.mean(batch_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')\n",
    "    \n",
    "# Print the predicted weights and bias\n",
    "print('')\n",
    "print(f'w = {w}')\n",
    "print(f'b = {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb9d9b",
   "metadata": {},
   "source": [
    "### 1.2. Linear regression from scratch in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b5b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: 3.427608013153076\n",
      "2/3: 0.008017840795218945\n",
      "3/3: 6.859275163151324e-05\n",
      "\n",
      "w = tensor([ 1.9993, -3.3998], requires_grad=True)\n",
      "b = tensor([4.2002], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = torch.tensor([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = torch.normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = torch.matmul(X, w_true)+b_true+torch.normal(0, 0.01, [number_examples]) # [number_examples]?\n",
    "\n",
    "# Define a function to read the dataset in random batches\n",
    "def batch(X, y, batch_size):\n",
    "    \n",
    "    # Generate the indices for all the examples and shuffle them\n",
    "    number_examples = X.shape[0]\n",
    "    example_indices = list(range(number_examples))\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for i in range(0, number_examples, batch_size):\n",
    "        \n",
    "        # Get the indices of the (randomized) examples for one minibatch\n",
    "        batch_indices = example_indices[i:min(i+batch_size, number_examples)]\n",
    "        \n",
    "        # Return the input and output minibatch and continue the iteration in the function\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "\n",
    "# Initialize the weights and bias to recover, requiring the gradients to be computed\n",
    "w = torch.normal(0, 1, [number_features], requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = torch.zeros(number_epochs)\n",
    "        \n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minibatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for X_batch, y_batch in batch(X, y, batch_size):\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = torch.matmul(X_batch, w) + b\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = 0.5*(y_hat-y_batch)**2\n",
    "        \n",
    "        # Compute the gradient on l with respect to w and b\n",
    "        # (sum and not mean as the gradients will be divided by the batch size during the SGD)\n",
    "        l.sum().backward()\n",
    "        \n",
    "        # Temporarily sets all of the requires_grad flags to false\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Save the mean loss for the current minibatch\n",
    "            batch_loss.append(l.mean())\n",
    "            \n",
    "            # Update the weights and bias using stochastic gradient descent\n",
    "            # (use augmented assignments to avoid modifying existing variables)\n",
    "            w -= lr*w.grad/len(l)\n",
    "            b -= lr*b.grad/len(l)\n",
    "            \n",
    "            # Set the gradients to zeros to avoid accumulating gradients\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = sum(batch_loss)/len(batch_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')\n",
    "    \n",
    "# Print the predicted weights and bias\n",
    "print('')\n",
    "print(f'w = {w}')\n",
    "print(f'b = {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27c36f",
   "metadata": {},
   "source": [
    "### 1.3. Linear regression using APIs in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "219ae16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: 2.8696396350860596\n",
      "2/3: 0.00011544800509000197\n",
      "3/3: 0.0001040133647620678\n",
      "\n",
      "w = tensor([[ 2.0003, -3.3997]])\n",
      "b = tensor([4.2010])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = torch.tensor([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs and derive outputs\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = torch.normal(0, 1, (number_examples, number_features))\n",
    "y = torch.matmul(X, w_true)+b_true+torch.normal(0, 0.01, [number_examples]) # [number_examples]?\n",
    "\n",
    "# Define a function to read the dataset in random batches\n",
    "def batch(X, y, batch_size):\n",
    "    \n",
    "    # Construct a PyTorch data iterator (?)\n",
    "    data_set = data.TensorDataset(*(X, y))\n",
    "    return data.DataLoader(data_set, batch_size, shuffle=True)\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "\n",
    "# Define the model (as a stack of layers) and add a fully-connected layer\n",
    "model = nn.Sequential(nn.Linear(number_features, 1))\n",
    "\n",
    "# Initialize the parameters\n",
    "model[0].weight.data.normal_(0, 0.01)\n",
    "model[0].bias.data.fill_(0)\n",
    "\n",
    "# Define the loss function (mean squared error, without the 0.5 factor)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Define the optimization algorithm (stochastic gradient descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = torch.zeros(number_epochs)\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minibatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for X_batch, y_batch in batch(X, y, batch_size):\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = model(X_batch)\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = loss(y_hat, y_batch[:, None])\n",
    "        \n",
    "        # Save the loss for the current minibatch (no with torch.no_grad())\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        # Set the gradients to zero (.zero_grad()?)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Computes the gradient (no .sum?)\n",
    "        l.backward()\n",
    "        \n",
    "        # Performs a single parameter update\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = sum(batch_loss)/len(batch_loss)\n",
    "        \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')\n",
    "    \n",
    "# Print the predicted weights and bias\n",
    "print('')\n",
    "print(f'w = {model[0].weight.data}')\n",
    "print(f'b = {model[0].bias.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d77f99",
   "metadata": {},
   "source": [
    "### 1.4. Linear regression using higher-level APIs in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef4ff12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 0s 526us/step - loss: 2.8988\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 327us/step - loss: 1.0193e-04\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 297us/step - loss: 9.6278e-05\n",
      "\n",
      "w = [[ 1.9999057]\n",
      " [-3.3999813]]\n",
      "b = [4.200842]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = np.array([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs and derive outputs\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = np.random.default_rng().normal(0, 1, (number_examples, number_features))\n",
    "y = np.matmul(X, w_true)+b_true+np.random.default_rng().normal(0, 0.01, number_examples)\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "lr = 0.03\n",
    "\n",
    "# Define the model (as a stack of layers) and add a densely-connected NN layer with initialized parameters\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, kernel_initializer=tf.initializers.RandomNormal(mean=0, stddev=0.01), \\\n",
    "                                                   bias_initializer='zeros')])\n",
    "\n",
    "# Configure the model for training with stochastic gradient descent optimizer and mean squared error loss\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
    "              loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# Train the model given the batch size and number of epochs\n",
    "model.fit(x=X, y=y, batch_size=batch_size, epochs=number_epochs, verbose=1)\n",
    "\n",
    "# Print the predicted weights and bias\n",
    "print('')\n",
    "print(f'w = {model.get_weights()[0]}')\n",
    "print(f'b = {model.get_weights()[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5ccf99",
   "metadata": {},
   "source": [
    "## 2. Softmax Regression\n",
    "https://d2l.ai/chapter_linear-networks/softmax-regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fce402",
   "metadata": {},
   "source": [
    "### 2.1. Softmax regression from scratch in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3672ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10: 2.152900399157075\n",
      "2/10: 2.1528876740472875\n",
      "3/10: 2.152915281652977\n",
      "4/10: 2.152878180835081\n",
      "5/10: 2.1529124906719153\n",
      "6/10: 2.152913458166309\n",
      "7/10: 2.1528916592550185\n",
      "8/10: 2.1529073294142425\n",
      "9/10: 2.1528966850975335\n",
      "10/10: 2.152916965646823\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16220/407588355.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m# Print the predicted weights and bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'w = {w}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'b = {b}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Get the data as train and test inputs and outputs\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "number_train = len(X_train)\n",
    "number_test = len(X_test)\n",
    "\n",
    "# Flatten and normalize the inputs\n",
    "input_size = np.size(X_train[0])\n",
    "X_train = np.reshape(X_train, (number_train, input_size))/255\n",
    "X_test = np.reshape(X_test, (number_test, input_size))/255\n",
    "\n",
    "# Turn the outputs into one-hot vectors\n",
    "output_size = 10\n",
    "y_train1 = np.zeros((number_train, output_size))\n",
    "y_test1 = np.zeros((number_test, output_size))\n",
    "for i in range(number_train): y_train1[i, y_train[i]] = 1\n",
    "for i in range(number_test): y_test1[i, y_test[i]] = 1\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 10\n",
    "batch_size = 256\n",
    "lr = 0.1\n",
    "\n",
    "# Initialize the weights and bias to recover\n",
    "W = np.random.default_rng().normal(0, 0.01, size=(input_size, output_size)) # 0.01?\n",
    "b = np.zeros(output_size)\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = np.zeros(number_epochs)\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Generate the indices for all the examples and shuffle them\n",
    "    train_indices = np.arange(number_train)\n",
    "    random.shuffle(train_indices)\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minbatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for j in np.arange(0, number_train, batch_size):\n",
    "        \n",
    "        # Get the indices of the (randomized) examples for one minibatch\n",
    "        batch_indices = train_indices[j:min(j+batch_size, number_train)]\n",
    "        \n",
    "        # Get the inputs and outputs for the current minibatch\n",
    "        X_batch = X_train[batch_indices, :]\n",
    "        y_batch = y_train1[batch_indices]\n",
    "        \n",
    "        # Compute the predicted outputs (logits)\n",
    "        o = np.matmul(X_batch, W) + b\n",
    "        \n",
    "        # Compute the softmax of the logits (indirectly to avoid numerical stability issues)\n",
    "        o = o-np.max(o, axis=1)[:, np.newaxis]\n",
    "        o_exp = np.exp(o)\n",
    "        y_hat = o_exp/np.sum(o_exp, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Compute the mean cross-entropy loss over the minibatch\n",
    "        l = np.mean(np.log(np.sum(o_exp, axis=1)-np.sum(y_batch*o, axis=1)))\n",
    "        \n",
    "        # Save the loss for the current minibatch\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        # HERE!!!\n",
    "        dl = y_hat-y_batch\n",
    "        \n",
    "        W = W-lr*\n",
    "        \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = np.mean(batch_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d000c99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37248a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294bdf78",
   "metadata": {},
   "source": [
    "### 2.2. Softmax regression from scratch in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1fded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils import data\n",
    "\n",
    "# Get the dataset (convert the data to 32-bit floating point tensors in [0, 1])\n",
    "fmnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "fmnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "# Define the labels\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07a9c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i[1] for i in fmnist_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be2ae504",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mnist_reader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11936/2065617658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmnist_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/fashion'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/fashion'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m't10k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mnist_reader'"
     ]
    }
   ],
   "source": [
    "import mnist_reader\n",
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ba7ded",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11936/2928817377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfmnist_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "X, y = data.DataLoader(fmnist_train, batch_size=1, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
