{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.1 Create a function (single neuron) with some inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the true weights and bias for a single neuron\n",
    "# (see http://d2l.ai/chapter_linear-networks/linear-regression-scratch.html)\n",
    "true_weights = np.array([2, -3.4])\n",
    "true_bias = 4.2\n",
    "\n",
    "# Create some inputs using a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(true_weights)\n",
    "true_inputs = np.random.normal(loc=0.0, scale=1.0, size=(number_examples, number_features))\n",
    "\n",
    "# Create some noise for all the examples using a normal distribution\n",
    "true_noise = np.random.normal(loc=0.0, scale=0.01, size=number_examples)\n",
    "    \n",
    "# Compute the true outputs using the inputs, and the true weights, bias, and noise\n",
    "true_outputs = np.matmul(true_inputs, true_weights) + true_bias + true_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.2. Learn the parameters of the neuron using gradient descent from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; loss: 2.8158391659305777\n",
      "Epoch: 1; loss: 0.008063429559107032\n",
      "Epoch: 2; loss: 7.47349006122696e-05\n",
      "Epoch: 3; loss: 5.2165294669915916e-05\n",
      "Epoch: 4; loss: 5.212232965254876e-05\n",
      "Epoch: 5; loss: 5.212337552022014e-05\n",
      "Epoch: 6; loss: 5.2123439776608544e-05\n",
      "Epoch: 7; loss: 5.2123443155928826e-05\n",
      "Epoch: 8; loss: 5.212344333184311e-05\n",
      "Epoch: 9; loss: 5.212344334097856e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize the training parameters\n",
    "number_epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.03\n",
    "\n",
    "# Initialize the predicted weights and bias\n",
    "predicted_weights = np.random.normal(loc=0.0, scale=0.01, size=number_features)\n",
    "predicted_bias = 0\n",
    "\n",
    "# Initialize the loss for all the batches\n",
    "predicted_loss = np.zeros(int(np.ceil(number_examples/batch_size)))\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Loop over the batches\n",
    "    k = 0\n",
    "    for j in range(0, number_examples, batch_size):\n",
    "        \n",
    "        # Derive the end index of the batch\n",
    "        j2 = min(j+100, number_examples)\n",
    "    \n",
    "        # Compute the predicted outputs using the inputs, and the predicted weights and bias\n",
    "        predicted_outputs = np.matmul(true_inputs[j:j2, :], predicted_weights) + predicted_bias\n",
    "\n",
    "        # Compute the difference between the predicted outputs and the true outputs\n",
    "        difference_outputs = predicted_outputs-true_outputs[j:j2]\n",
    "\n",
    "        # Compute the loss using the squared error\n",
    "        predicted_loss[k] = np.mean(0.5*np.power(difference_outputs, 2))\n",
    "\n",
    "        # Update the predicted weights and bias using gradient descent, taking the derivative of the loss function\n",
    "        predicted_weights = predicted_weights-learning_rate*np.mean(true_inputs[j:j2, :]*(difference_outputs)[:, np.newaxis], axis=0)\n",
    "        predicted_bias = predicted_bias-learning_rate*np.mean(difference_outputs, axis=0)\n",
    "        \n",
    "        # Update the index\n",
    "        k = k+1\n",
    "        \n",
    "    # Print the epoch and loss\n",
    "    print(f\"Epoch: {i}; loss: {np.mean(predicted_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.3. Learn the parameters of the neuron using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "number_epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.03\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(np.shape(true_inputs)[1],)))\n",
    "model.add(tf.keras.layers.Dense(1, activation=None, kernel_initializer=tf.initializers.RandomNormal(mean=0, stddev=0.01)))\n",
    "opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.2. Learn the parameters of the neuron using an evolutionary algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; loss: 2.744160890040827\n",
      "Epoch: 1; loss: 0.0005916940774964454\n",
      "Epoch: 2; loss: 0.00022320956614667802\n",
      "Epoch: 3; loss: 0.00013476951981175844\n",
      "Epoch: 4; loss: 0.0001065308802221738\n",
      "Epoch: 5; loss: 9.845686483952831e-05\n",
      "Epoch: 6; loss: 9.7595533402721e-05\n",
      "Epoch: 7; loss: 9.737403067618305e-05\n",
      "Epoch: 8; loss: 9.75313417395032e-05\n",
      "Epoch: 9; loss: 9.747489960243033e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize the training parameters\n",
    "number_epochs = 10\n",
    "batch_size = 10\n",
    "number_individuals = 10\n",
    "number_parents = 1\n",
    "mutation_rate = 0.2\n",
    "\n",
    "# Initialize the predicted weights and bias\n",
    "predicted_weights = np.random.normal(loc=0.0, scale=0.01, size=(number_features, number_individuals))\n",
    "predicted_bias = np.zeros((1, number_individuals))\n",
    "\n",
    "# Initialize the loss for all the batches and individuals\n",
    "predicted_loss = np.zeros((int(np.ceil(number_examples/batch_size)), number_individuals))\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Loop over the batches\n",
    "    k = 0\n",
    "    for j in range(0, number_examples, batch_size):\n",
    "        \n",
    "        # Derive the end index of the batch\n",
    "        j2 = min(j+100, number_examples)\n",
    "    \n",
    "        # Compute the predicted outputs using the inputs, and the predicted weights and bias, for every individual\n",
    "        predicted_outputs = np.matmul(true_inputs[j:j2, :], predicted_weights) + predicted_bias\n",
    "\n",
    "        # Compute the loss using the squared error, for every individual\n",
    "        predicted_loss[k, :] = np.mean(np.power(predicted_outputs-true_outputs[j:j2, np.newaxis], 2), axis=0)\n",
    "        \n",
    "        # Get the indices of the parents, the fittest individuals\n",
    "        parent_indices = np.argsort(predicted_loss[k, :])[0:number_parents]\n",
    "        \n",
    "        # Compute the mutation scale\n",
    "        mutation_scale = mutation_rate*np.mean(predicted_loss[k, parent_indices])\n",
    "        \n",
    "        # Update the predicted weights and bias using evolutionary algorithm, doing crossover and mutation\n",
    "        predicted_weights = np.mean(predicted_weights[:, parent_indices], axis=1)[:, np.newaxis] \\\n",
    "        + np.random.normal(loc=0.0, scale=mutation_scale, size=(number_features, number_individuals))\n",
    "        predicted_bias = np.mean(predicted_bias[:, parent_indices]) \\\n",
    "        + np.random.normal(loc=0.0, scale=mutation_scale, size=(1, number_individuals))\n",
    "        \n",
    "        # Update the index\n",
    "        k = k+1\n",
    "        \n",
    "    # Print the epoch and loss\n",
    "    print(f\"Epoch: {i}; loss: {np.mean(predicted_loss)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
