{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.1 Create a function (single neuron) with some inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the true weights and bias for a single neuron\n",
    "# (see also http://d2l.ai/chapter_linear-networks/linear-regression-scratch.html)\n",
    "true_weights = np.array([2, -3.4])\n",
    "true_bias = 4.2\n",
    "\n",
    "# Create some inputs using a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(true_weights)\n",
    "true_inputs = np.random.normal(loc=0.0, scale=1.0, size=(number_examples, number_features))\n",
    "\n",
    "# Create some noise for all the examples using a normal distribution\n",
    "true_noise = np.random.normal(loc=0.0, scale=0.01, size=number_examples)\n",
    "    \n",
    "# Compute the true outputs using the inputs, and the true weights, bias, and noise\n",
    "true_outputs = np.matmul(true_inputs, true_weights) + true_bias + true_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.2. Learn the parameters of the neuron using gradient descent from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; loss: 2.8037546806144213\n",
      "Epoch: 1; loss: 0.00927178390146684\n",
      "Epoch: 2; loss: 8.412635819532869e-05\n",
      "Epoch: 3; loss: 5.009629749576156e-05\n",
      "Epoch: 4; loss: 4.993365863470035e-05\n",
      "Epoch: 5; loss: 4.993087456283108e-05\n",
      "Epoch: 6; loss: 4.99307166890457e-05\n",
      "Epoch: 7; loss: 4.993070621734263e-05\n",
      "Epoch: 8; loss: 4.9930705525756386e-05\n",
      "Epoch: 9; loss: 4.993070548048438e-05\n",
      "\n",
      "Predicted weights: [ 1.99955669 -3.40039523]\n",
      "Predicted bias: 4.200038276450225\n"
     ]
    }
   ],
   "source": [
    "# Define the training parameters\n",
    "number_epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.03\n",
    "\n",
    "# Initialize the predicted weights and bias\n",
    "predicted_weights = np.random.normal(loc=0.0, scale=0.01, size=number_features)\n",
    "predicted_bias = 0\n",
    "\n",
    "# Initialize the loss for all the batches\n",
    "predicted_loss = np.zeros(int(np.ceil(number_examples/batch_size)))\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Loop over the batches\n",
    "    k = 0\n",
    "    for j in range(0, number_examples, batch_size):\n",
    "        \n",
    "        # Derive the end index of the batch\n",
    "        j2 = min(j+100, number_examples)\n",
    "    \n",
    "        # Compute the predicted outputs using the inputs, and the predicted weights and bias\n",
    "        predicted_outputs = np.matmul(true_inputs[j:j2, :], predicted_weights) + predicted_bias\n",
    "\n",
    "        # Compute the difference between the predicted outputs and the true outputs\n",
    "        difference_outputs = predicted_outputs-true_outputs[j:j2]\n",
    "\n",
    "        # Compute the loss using the mean squared error\n",
    "        predicted_loss[k] = np.mean(0.5*np.power(difference_outputs, 2))\n",
    "\n",
    "        # Update the predicted weights and bias using gradient descent, taking the derivative of the loss function\n",
    "        predicted_weights = predicted_weights-learning_rate*np.mean(true_inputs[j:j2, :]*(difference_outputs)[:, np.newaxis], axis=0)\n",
    "        predicted_bias = predicted_bias-learning_rate*np.mean(difference_outputs, axis=0)\n",
    "        \n",
    "        # Update the index\n",
    "        k = k+1\n",
    "        \n",
    "    # Print the epoch and loss\n",
    "    print(f\"Epoch: {i}; loss: {np.mean(predicted_loss)}\")\n",
    "    \n",
    "# Print the predicted weights and bias\n",
    "print(\"\")\n",
    "print(f\"Predicted weights: {predicted_weights}\")\n",
    "print(f\"Predicted bias: {predicted_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.3. Learn the parameters of the neuron using gradient descent in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 2.8682\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.2676e-04\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.0245e-04\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.0302e-04\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.0295e-04\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.0251e-04\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 1.0245e-04\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.0280e-04\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.0253e-04\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 1.0268e-04\n",
      "\n",
      "Predicted weights: [ 2.000095  -3.3992987]\n",
      "Predicted bias: 4.200600624084473\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the training parameters\n",
    "number_epochs = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.03\n",
    "\n",
    "# Initialize the model (as a feedforward NN)\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an input with the number of features\n",
    "model.add(tf.keras.Input(shape=number_features))\n",
    "\n",
    "# Add a densely-connected NN layer without activation and with initialized weights and bias\n",
    "model.add(tf.keras.layers.Dense(1, activation=None, \\\n",
    "                                kernel_initializer=tf.initializers.RandomNormal(mean=0, stddev=0.01), \\\n",
    "                                bias_initializer=\"zeros\"))\n",
    "\n",
    "# Configure the model for training with gradient descent optimizer and mean squared error loss\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate), loss=\"mean_squared_error\")\n",
    "\n",
    "# Train the model give the batch size and number of epochs\n",
    "model.fit(x=true_inputs, y=true_outputs, batch_size=batch_size, epochs=number_epochs, verbose=1)\n",
    "\n",
    "# Print the predicted weights and bias\n",
    "print(\"\")\n",
    "print(f\"Predicted weights: {model.get_weights()[0][:, 0]}\")\n",
    "print(f\"Predicted bias: {model.get_weights()[1][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01.2. Learn the parameters of the neuron using an evolutionary algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; loss: 10.126777128069596\n",
      "Epoch: 1; loss: 0.0005799359993380929\n",
      "Epoch: 2; loss: 0.00020963962076826672\n",
      "Epoch: 3; loss: 0.00013234465227642445\n",
      "Epoch: 4; loss: 0.00010788320805971668\n",
      "Epoch: 5; loss: 0.00010101878563914387\n",
      "Epoch: 6; loss: 9.998486872936858e-05\n",
      "Epoch: 7; loss: 0.00010015609962209404\n",
      "Epoch: 8; loss: 0.00010006813528823452\n",
      "Epoch: 9; loss: 9.992032914625884e-05\n",
      "\n",
      "Predicted weights: [ 1.99979901 -3.39991128]\n",
      "Predicted bias: 4.2000524349572945\n"
     ]
    }
   ],
   "source": [
    "# Initialize the training parameters\n",
    "number_epochs = 10\n",
    "batch_size = 10\n",
    "number_individuals = 10\n",
    "number_parents = 1\n",
    "mutation_rate = 0.2\n",
    "\n",
    "# Initialize the predicted weights and bias\n",
    "predicted_weights = np.random.normal(loc=0.0, scale=0.01, size=(number_features, number_individuals))\n",
    "predicted_bias = np.zeros((1, number_individuals))\n",
    "\n",
    "# Initialize the loss for all the batches and individuals\n",
    "predicted_loss = np.zeros((int(np.ceil(number_examples/batch_size)), number_individuals))\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Loop over the batches\n",
    "    k = 0\n",
    "    for j in range(0, number_examples, batch_size):\n",
    "        \n",
    "        # Derive the end index of the batch\n",
    "        j2 = min(j+100, number_examples)\n",
    "    \n",
    "        # Compute the predicted outputs using the inputs, and the predicted weights and bias, for every individual\n",
    "        predicted_outputs = np.matmul(true_inputs[j:j2, :], predicted_weights) + predicted_bias\n",
    "\n",
    "        # Compute the loss using the squared error, for every individual\n",
    "        predicted_loss[k, :] = np.mean(np.power(predicted_outputs-true_outputs[j:j2, np.newaxis], 2), axis=0)\n",
    "        \n",
    "        # Get the indices of the parents, the fittest individuals\n",
    "        parent_indices = np.argsort(predicted_loss[k, :])[0:number_parents]\n",
    "        \n",
    "        # Compute the mutation scale\n",
    "        mutation_scale = mutation_rate*np.mean(predicted_loss[k, parent_indices])\n",
    "        \n",
    "        # Update the predicted weights and bias using evolutionary algorithm, doing crossover and mutation\n",
    "        predicted_weights = np.mean(predicted_weights[:, parent_indices], axis=1)[:, np.newaxis] \\\n",
    "        + np.random.normal(loc=0.0, scale=mutation_scale, size=(number_features, number_individuals))\n",
    "        predicted_bias = np.mean(predicted_bias[:, parent_indices]) \\\n",
    "        + np.random.normal(loc=0.0, scale=mutation_scale, size=(1, number_individuals))\n",
    "        \n",
    "        # Update the index\n",
    "        k = k+1\n",
    "        \n",
    "    # Print the epoch and loss\n",
    "    print(f\"Epoch: {i}; loss: {np.mean(predicted_loss)}\")\n",
    "    \n",
    "# Print the predicted weights and bias\n",
    "print(\"\")\n",
    "print(f\"Predicted weights: {np.mean(predicted_weights, axis=1)}\")\n",
    "print(f\"Predicted bias: {np.mean(predicted_bias)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
