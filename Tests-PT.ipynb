{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd998c20",
   "metadata": {},
   "source": [
    "# Deep Learning Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28588085",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "https://d2l.ai/chapter_linear-networks/linear-regression-scratch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eab6bb",
   "metadata": {},
   "source": [
    "### 1.1. Linear regression from scratch with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a84b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: 34.28472425766261\n",
      "2/3: 0.12353325147705162\n",
      "3/3: 0.0008822978982950066\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = np.array([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = np.random.default_rng().normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = np.matmul(X, w_true)+b_true+np.random.default_rng().normal(0, 0.01, number_examples)\n",
    "\n",
    "# Define the parameters for the training\n",
    "lr = 0.03\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize the weights and bias to recover\n",
    "w = np.random.default_rng().normal(0, 1, number_features)\n",
    "b = 0\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = np.zeros(number_epochs)\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Generate the indices for all the examples and shuffle them\n",
    "    example_indices =  np.arange(number_examples)\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minbatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for j in np.arange(0, number_examples, batch_size):\n",
    "        \n",
    "        # Get the indices of the (randomized) examples for one minibatch\n",
    "        batch_indices = example_indices[j:min(j+batch_size, number_examples)]\n",
    "        \n",
    "        # Get the inputs and outputs for the current minibatch\n",
    "        X_batch = X[batch_indices, :]\n",
    "        y_batch = y[batch_indices]\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = np.matmul(X_batch, w) + b\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = np.mean(0.5*np.power(y_hat-y_batch, 2))\n",
    "        \n",
    "        # Save the mean loss for the current minibatch\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        # Update the weights and bias using stochastic gradient descent\n",
    "        w = w - lr*np.mean(X_batch*(y_hat-y_batch)[:, np.newaxis], axis=0)\n",
    "        b = b - lr*np.mean(y_hat-y_batch, axis=0)\n",
    "        \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = np.mean(batch_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb9d9b",
   "metadata": {},
   "source": [
    "### 1.2. Linear regression from scratch with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01b5b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = torch.tensor([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = torch.normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = torch.matmul(X, w_true)+b_true+torch.normal(0, 0.01, [number_examples]) # ?\n",
    "\n",
    "# Define the parameters for the training\n",
    "lr = 0.03\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize the weights and bias to recover, requiring the gradients to be computed\n",
    "w = torch.normal(0, 1, [number_features], requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = torch.zeros(number_epochs)\n",
    "\n",
    "# Define a function to read the dataset in random batches\n",
    "def batch(X, y, batch_size):\n",
    "    \n",
    "    # Generate the indices for all the examples and shuffle them\n",
    "    number_examples = X.shape[0]\n",
    "    example_indices = list(range(number_examples))\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for i in range(0, number_examples, batch_size):\n",
    "        \n",
    "        # Get the indices of the (randomized) examples for one minibatch\n",
    "        batch_indices = example_indices[i:min(i+batch_size, number_examples)]\n",
    "        \n",
    "        # Return the input and output minibatches and continue the iteration in the function\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "        \n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for X_batch, y_batch in batch(X, y, batch_size):\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = torch.matmul(X_batch, w) + b\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = 0.5*(y_hat-y_batch)**2\n",
    "        \n",
    "        # Compute the gradient on l with respect to w and b\n",
    "        l.sum().backward() # ?\n",
    "        \n",
    "#         # Save the mean loss for the current minibatch\n",
    "#         batch_loss.append(l)\n",
    "        \n",
    "#         # Update the weights and bias using stochastic gradient descent\n",
    "#         w = w - lr*np.mean(X_batch*(y_hat-y_batch)[:, np.newaxis], axis=0)\n",
    "#         b = b - lr*np.mean(y_hat-y_batch, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7703213d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e32b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ec59de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f802f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21c9ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a94d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcb40049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1802.0082, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5abdcd9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8192.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "219ae16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8192., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d/a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
