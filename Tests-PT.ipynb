{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd998c20",
   "metadata": {},
   "source": [
    "# Deep Learning Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28588085",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "https://d2l.ai/chapter_linear-networks/linear-regression-scratch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eab6bb",
   "metadata": {},
   "source": [
    "### 1.1. Linear regression from scratch in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a84b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: 2.877929794325924\n",
      "2/3: 0.00457104134012375\n",
      "3/3: 5.3823265050773114e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = np.array([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = np.random.default_rng().normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = np.matmul(X, w_true)+b_true+np.random.default_rng().normal(0, 0.01, number_examples)\n",
    "\n",
    "# Define the parameters for the training\n",
    "lr = 0.03\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize the weights and bias to recover\n",
    "w = np.random.default_rng().normal(0, 1, number_features)\n",
    "b = 0\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = np.zeros(number_epochs)\n",
    "\n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Generate the indices for all the examples and shuffle them\n",
    "    example_indices =  np.arange(number_examples)\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minbatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for j in np.arange(0, number_examples, batch_size):\n",
    "        \n",
    "        # Get the indices of the (randomized) examples for one minibatch\n",
    "        batch_indices = example_indices[j:min(j+batch_size, number_examples)]\n",
    "        \n",
    "        # Get the inputs and outputs for the current minibatch\n",
    "        X_batch = X[batch_indices, :]\n",
    "        y_batch = y[batch_indices]\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = np.matmul(X_batch, w) + b\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = np.mean(0.5*np.power(y_hat-y_batch, 2))\n",
    "        \n",
    "        # Save the mean loss for the current minibatch\n",
    "        batch_loss.append(l)\n",
    "        \n",
    "        # Update the weights and bias using stochastic gradient descent\n",
    "        w = w - lr*np.mean(X_batch*(y_hat-y_batch)[:, np.newaxis], axis=0)\n",
    "        b = b - lr*np.mean(y_hat-y_batch, axis=0)\n",
    "        \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = np.mean(batch_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb9d9b",
   "metadata": {},
   "source": [
    "### 1.2. Linear regression from scratch in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01b5b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3: 4.321141719818115\n",
      "2/3: 0.014720503240823746\n",
      "3/3: 0.00010053945879917592\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = torch.tensor([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = torch.normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = torch.matmul(X, w_true)+b_true+torch.normal(0, 0.01, [number_examples]) # [number_examples]?\n",
    "\n",
    "# Define a function to read the dataset in random batches\n",
    "def batch(X, y, batch_size):\n",
    "    \n",
    "    # Generate the indices for all the examples and shuffle them\n",
    "    number_examples = X.shape[0]\n",
    "    example_indices = list(range(number_examples))\n",
    "    random.shuffle(example_indices)\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for i in range(0, number_examples, batch_size):\n",
    "        \n",
    "        # Get the indices of the (randomized) examples for one minibatch\n",
    "        batch_indices = example_indices[i:min(i+batch_size, number_examples)]\n",
    "        \n",
    "        # Return the input and output minibatch and continue the iteration in the function\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "# Define the parameters for the training\n",
    "lr = 0.03\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize the weights and bias to recover, requiring the gradients to be computed\n",
    "w = torch.normal(0, 1, [number_features], requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = torch.zeros(number_epochs)\n",
    "        \n",
    "# Initialize an array for the mean loss over the minibatches of every epoch\n",
    "epoch_loss = torch.zeros(number_epochs)\n",
    "        \n",
    "# Loop over the epochs\n",
    "for i in range(number_epochs):\n",
    "    \n",
    "    # Initialize a list for the mean loss over the examples of every minibatch\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for X_batch, y_batch in batch(X, y, batch_size):\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = torch.matmul(X_batch, w) + b\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = 0.5*(y_hat-y_batch)**2\n",
    "        \n",
    "        # Compute the gradient on l with respect to w and b\n",
    "        # (sum and not mean as the gradients will be divided by the batch size during the SGD)\n",
    "        l.sum().backward()\n",
    "        \n",
    "        # Temporarily sets all of the requires_grad flags to false\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Save the mean loss for the current minibatch\n",
    "            batch_loss.append(l.mean())\n",
    "            \n",
    "            # Update the weights and bias using stochastic gradient descent\n",
    "            # (use augmented assignments to avoid modifying existing variables)\n",
    "            w -= lr*w.grad/len(l)\n",
    "            b -= lr*b.grad/len(l)\n",
    "            \n",
    "            # Set the gradients to zeros to avoid accumulating gradients\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    # Update the mean loss for the current epoch\n",
    "    epoch_loss[i] = sum(batch_loss)/len(batch_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print(f'{i+1}/{number_epochs}: {epoch_loss[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27c36f",
   "metadata": {},
   "source": [
    "### 1.3. Linear regression using APIs in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "219ae16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 14.819270\n",
      "epoch 2, loss 14.819186\n",
      "epoch 3, loss 14.819188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zrafii\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\zrafii\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([1000])) that is different to the input size (torch.Size([1000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "\n",
    "# Define the true weights and bias of the model\n",
    "w_true = torch.tensor([2, -3.4])\n",
    "b_true = 4.2\n",
    "\n",
    "# Generate inputs, sampled from a standard normal distribution\n",
    "number_examples = 1000\n",
    "number_features = len(w_true)\n",
    "X = torch.normal(0, 1, (number_examples, number_features))\n",
    "\n",
    "# Derive the outputs, with some noise\n",
    "y = torch.matmul(X, w_true)+b_true+torch.normal(0, 0.01, [number_examples]) # [number_examples]?\n",
    "\n",
    "# Define a function to read the dataset in random batches\n",
    "def batch(X, y, batch_size):\n",
    "    \n",
    "    # Construct a PyTorch data iterator (?)\n",
    "    data_set = data.TensorDataset(*(X, y))\n",
    "    return data.DataLoader(data_set, batch_size, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "# - Sequential class: defines container to chain several layers together\n",
    "# - Linear class: defines fully-connected layer (with input and output feature dimensions)\n",
    "model = nn.Sequential(nn.Linear(number_features, 1))\n",
    "\n",
    "# Initialize the parameters\n",
    "# - model[0]: accessing first layer in the model\n",
    "# - weight.data and bias.data to access the parameters\n",
    "model[0].weight.data.normal_(0, 0.01)\n",
    "model[0].bias.data.fill_(0)\n",
    "\n",
    "# Define the loss function (mean squared error, without the 0.5 factor)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Define the optimization algorithm (stochastic gradient descent)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)\n",
    "\n",
    "# Define the parameters for the training\n",
    "number_epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(number_epochs):\n",
    "    \n",
    "    # Loop over the examples in batches\n",
    "    for X_batch, y_batch in batch(X, y, batch_size):\n",
    "        \n",
    "        # Compute the predicted outputs\n",
    "        y_hat = model(X_batch)\n",
    "        \n",
    "        # Compute the loss between the predicted and true outputs\n",
    "        l = loss(y_hat, y)\n",
    "        \n",
    "        # HERE!!!\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    l = loss(model(X), y)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e904fbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f069425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
